{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0RJ22Fzp6eq",
        "outputId": "3f98ee79-7075-4318-fc3e-57782b5c4c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/danieldu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "wget: /home/danieldu/anaconda3/lib/libuuid.so.1: no version information available (required by wget)\n",
            "--2023-10-12 19:22:38--  https://storage.googleapis.com/learning-datasets/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.42.251, 172.217.163.59, 172.217.160.123, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.42.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: ‘/tmp/sarcasm.json’\n",
            "\n",
            "/tmp/sarcasm.json   100%[===================>]   5.38M  3.67MB/s    in 1.5s    \n",
            "\n",
            "2023-10-12 19:22:40 (3.67 MB/s) - ‘/tmp/sarcasm.json’ saved [5643545/5643545]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zcT8Xu2QP-zV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zbOT4-5WD-Gv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "dataset = []\n",
        "label_dataset = []\n",
        "for item in datastore:\n",
        "    dataset.append(item[\"headline\"])\n",
        "    label_dataset.append(item[\"is_sarcastic\"])\n",
        "\n",
        "dataset = np.array(dataset)\n",
        "label_dataset = np.array(label_dataset)\n",
        "train_size = 0.8\n",
        "size = int(len(dataset) * train_size)\n",
        "\n",
        "train_sentence = dataset[:size]\n",
        "test_sentence = dataset[size:]\n",
        "\n",
        "train_label = label_dataset[:size]\n",
        "test_label = label_dataset[size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vk7m6w8wPp77"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(train_sentence)\n",
        "embedding_size = 64\n",
        "max_length = 25\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_sentence)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentence)\n",
        "padded_train_sequences = pad_sequences(train_sequences, maxlen=max_length, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentence)\n",
        "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2JbDK6nnXB60"
      },
      "outputs": [],
      "source": [
        "units = 128\n",
        "embedding_size = 100\n",
        "vocab_size = len(tokenizer.index_word) + 1\n",
        "input_length = max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fMM_TEqmVctb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-12 19:22:42.267804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.267996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.293289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.293485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.293636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.293782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.294411: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-12 19:22:42.437695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.437879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.438026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.438162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.438296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.438431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.832789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.832989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.833138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.833281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.833422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.833555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2724 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
            "2023-10-12 19:22:42.833861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-10-12 19:22:42.833988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6776 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "model_lstm = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocab_size, 64),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(32, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "#https://www.kaggle.com/code/nikhilkohli/sarcasm-detection-using-lstm-gru-85-accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-G-eIXlp3CY",
        "outputId": "4e0ea9e2-f13a-4155-aee3-0c28cbdfb20c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
        "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model_lstm.fit(\n",
        "    padded_train_sequences, train_label,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    validation_data=(padded_test_sequences, test_label),\n",
        "    callbacks=[rlrp] ,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWVZK-8hnovk",
        "outputId": "b72e5c50-0254-4a13-9b9e-18dd4ed3e377"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model_lstm.save(\"sarcasm_model_lstm.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
